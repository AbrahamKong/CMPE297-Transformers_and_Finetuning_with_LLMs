{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahamKong/CMPE297-Transformers_and_Finetuning_with_LLMs/blob/main/nanogpt_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69acfc48",
      "metadata": {
        "id": "69acfc48"
      },
      "source": [
        "## Building a GPT with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3d7d42b9",
      "metadata": {
        "id": "3d7d42b9",
        "outputId": "9c103c56-7896-41ea-de31-a2d7384f42e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  102434\n",
            "First 1000 characters of the dataset:\n",
            "< Shakespeare -- THE COMEDY OF ERRORS >\n",
            "< from Online Library of Liberty (http://oll.libertyfund.org) >\n",
            "< Unicode .txt version by Mike Scott (http://www.lexically.net) >\n",
            "< from \"The Complete Works of William Shakespeare\" >\n",
            "< ed. with a glossary by W.J. Craig M.A. >\n",
            "< (London: Oxford University Press, 1916) >\n",
            "<STAGE DIR>\n",
            "<Scene.—Ephesus.>\n",
            "</STAGE DIR>\n",
            "\n",
            "\n",
            "<ACT 1>\n",
            "\n",
            "\n",
            "<SCENE 1>\n",
            "<A Hall in the Duke's Palace.>\n",
            "<STAGE DIR>\n",
            "<Enter Duke, Ægeon, Gaoler, Officers, and other Attendants.>\n",
            "</STAGE DIR>\n",
            "<ÆGEON>\t<1%>\n",
            "\tProceed, Solinus, to procure my fall,\n",
            "\tAnd by the doom of death end woes and all.\n",
            "</ÆGEON>\n",
            "\n",
            "<DUKE>\t<1%>\n",
            "\tMerchant of Syracusa, plead no more.\n",
            "\tI am not partial to infringe our laws:\n",
            "\tThe enmity and discord which of late\n",
            "\tSprung from the rancorous outrage of your duke\n",
            "\tTo merchants, our well-dealing countrymen,\n",
            "\tWho, wanting guilders to redeem their lives,\n",
            "\tHave seal'd his rigorous statutes with their bloods,\n",
            "\tExcludes all pity from our threat'ning looks.\n",
            "\tFor, since the mortal and intestin\n",
            "Unique characters in the dataset:\n",
            "\t\n",
            " !\"%'(),-./0123456789:;<>?ABCDEFGHIJKLMNOPQRSTUVWYZabcdefghijklmnopqrstuvwxyzÆæœ—\n",
            "Vocabulary Size: 83\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "# 1. Download the tiny shakespeare dataset\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/AbrahamKong/CMPE297-Transformers_and_Finetuning_with_LLMs/main/data/input.txt\"\n",
        "filename = \"input.txt\"\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# 2. Read the dataset into a string variable named 'text'\n",
        "with open('input.txt', 'r', encoding=\"utf16\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 3. Display the length of the dataset in characters\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "\n",
        "# 4. Inspect the first 1000 characters of the dataset\n",
        "print(\"First 1000 characters of the dataset:\")\n",
        "print(text[:1000])\n",
        "\n",
        "# 5. Identify and display all the unique characters in the dataset and determine the vocabulary size\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Unique characters in the dataset:\")\n",
        "print(''.join(chars))\n",
        "print(\"Vocabulary Size:\", vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1a52f315",
      "metadata": {
        "id": "1a52f315"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define char2idx and idx2char dictionaries based on the unique characters in the dataset\n",
        "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx2char = {idx: char for char, idx in char2idx.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6590fce9",
      "metadata": {
        "id": "6590fce9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# BigramLanguageModel class definition in TensorFlow\n",
        "class BigramLanguageModel(keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = layers.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def call(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "        if targets is not None:\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "            return logits, loss\n",
        "        return logits, None\n",
        "\n",
        "    # Text generation method\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        generated_tokens = []\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)\n",
        "            # Reshape logits to ensure it's of shape [batch_size, num_classes]\n",
        "            logits = tf.squeeze(logits, axis=1)\n",
        "            # Use softmax to get probabilities\n",
        "            probs = tf.nn.softmax(logits, axis=-1)\n",
        "            # Sample the next token\n",
        "            idx_next = tf.random.categorical(probs, num_samples=1, dtype=tf.int32)\n",
        "            generated_tokens.append(idx_next)\n",
        "            idx = idx_next\n",
        "        return tf.concat(generated_tokens, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "74d0fc77",
      "metadata": {
        "id": "74d0fc77"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Corrected BigramLanguageModel class definition in TensorFlow\n",
        "class BigramLanguageModel(keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = layers.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def call(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "        if targets is not None:\n",
        "            # Ensure logits and targets have the same sequence length\n",
        "            logits = logits[:, :-1, :]\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "            return logits, loss\n",
        "        return logits, None\n",
        "\n",
        "    # Text generation method remains the same as before\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        generated_tokens = []\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)\n",
        "            # Reshape logits to ensure it's of shape [batch_size, num_classes]\n",
        "            logits = tf.squeeze(logits, axis=1)\n",
        "            # Use softmax to get probabilities\n",
        "            probs = tf.nn.softmax(logits, axis=-1)\n",
        "            # Sample the next token\n",
        "            idx_next = tf.random.categorical(probs, num_samples=1, dtype=tf.int32)\n",
        "            generated_tokens.append(idx_next)\n",
        "            idx = idx_next\n",
        "        return tf.concat(generated_tokens, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "43c93c50",
      "metadata": {
        "id": "43c93c50"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data Batching\n",
        "\n",
        "# Function to generate batches of data for training\n",
        "def get_batch(data, batch_size, block_size):\n",
        "    # Convert data to indices\n",
        "    data_idx = [char2idx[char] for char in data]\n",
        "    data_idx = tf.constant(data_idx, dtype=tf.int32)\n",
        "\n",
        "    # Calculate the number of blocks\n",
        "    num_blocks = len(data_idx) // (batch_size * block_size)\n",
        "\n",
        "    # Truncate data to fit exactly into the number of blocks\n",
        "    data_idx = tf.reshape(data_idx[:(num_blocks * batch_size * block_size)], (batch_size, -1))\n",
        "\n",
        "    # Generate batches\n",
        "    for i in range(0, data_idx.shape[1], block_size):\n",
        "        x = data_idx[:, i:i+block_size]\n",
        "        y = data_idx[:, i+1:i+1+block_size-1]  # Adjusted to have sequence length of block_size - 1\n",
        "        yield x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a51fe818",
      "metadata": {
        "id": "a51fe818"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Training Setup\n",
        "\n",
        "# Define the loss function\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training step function\n",
        "@tf.function\n",
        "def train_step(model, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits, loss = model(x, targets=y)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Function to estimate average loss\n",
        "def estimate_loss(model, data, batch_size, block_size):\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    for x, y in get_batch(data, batch_size, block_size):\n",
        "        _, loss = model(x, targets=y)\n",
        "        total_loss += loss\n",
        "        total_batches += 1\n",
        "    return total_loss / total_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ea99948c",
      "metadata": {
        "id": "ea99948c",
        "outputId": "e8ad3457-a6a5-4473-f705-d2c80827c319",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Step 0, Loss: 4.419447898864746\n",
            "Epoch 2/5, Step 0, Loss: 4.400918483734131\n",
            "Epoch 3/5, Step 0, Loss: 4.382547378540039\n",
            "Epoch 4/5, Step 0, Loss: 4.364249229431152\n",
            "Epoch 5/5, Step 0, Loss: 4.346034526824951\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Model instantiation\n",
        "model = BigramLanguageModel(vocab_size=vocab_size)\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "block_size = 128\n",
        "epochs = 5\n",
        "\n",
        "# Main Training Loop\n",
        "for epoch in range(epochs):\n",
        "    # Iterate over the batches of the dataset\n",
        "    for step, (x, y) in enumerate(get_batch(text, batch_size, block_size)):\n",
        "        loss = train_step(model, x, y)\n",
        "\n",
        "        # Print loss every 100 steps\n",
        "        if step % 100 == 0:\n",
        "            if loss is not None:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Step {step}, Loss: {loss.numpy()}\")\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Step {step}, Loss not computed\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}